{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14592870,"sourceType":"datasetVersion","datasetId":9321449},{"sourceId":14643928,"sourceType":"datasetVersion","datasetId":9354574},{"sourceId":14700066,"sourceType":"datasetVersion","datasetId":9390990},{"sourceId":737760,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":562600,"modelId":575190},{"sourceId":738787,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":563439,"modelId":575960}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:10.746671Z","iopub.execute_input":"2026-02-02T12:27:10.747084Z","iopub.status.idle":"2026-02-02T12:27:10.752367Z","shell.execute_reply.started":"2026-02-02T12:27:10.747054Z","shell.execute_reply":"2026-02-02T12:27:10.751342Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"court = pd.read_csv(\"/kaggle/input/nlp-ass1-datasets/court.csv\", header=None)\nsocial = pd.read_csv(\"/kaggle/input/nlp-ass1-datasets/social.csv\", header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:10.754293Z","iopub.execute_input":"2026-02-02T12:27:10.755374Z","iopub.status.idle":"2026-02-02T12:27:12.003059Z","shell.execute_reply.started":"2026-02-02T12:27:10.755331Z","shell.execute_reply":"2026-02-02T12:27:12.001841Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"court_train, court_val = court[:80000], court[80000:]\nsocial_train, social_val = social[:80000],  social[80000:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.004222Z","iopub.execute_input":"2026-02-02T12:27:12.004523Z","iopub.status.idle":"2026-02-02T12:27:12.028852Z","shell.execute_reply.started":"2026-02-02T12:27:12.004496Z","shell.execute_reply":"2026-02-02T12:27:12.027846Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from nltk.tokenize import WordPunctTokenizer\n\nwpt = WordPunctTokenizer()","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:16:16.199112Z","start_time":"2026-01-23T12:16:12.047573Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.030041Z","iopub.execute_input":"2026-02-02T12:27:12.030368Z","iopub.status.idle":"2026-02-02T12:27:12.043525Z","shell.execute_reply.started":"2026-02-02T12:27:12.030335Z","shell.execute_reply":"2026-02-02T12:27:12.042660Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from collections import defaultdict\n\ndef build_word_freqs(corpus):\n    word_freqs = defaultdict(int)\n    for text in corpus:\n       for tok in wpt.tokenize(str(text)):\n           word_freqs[tok] += 1\n    return word_freqs","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:16:16.208951Z","start_time":"2026-01-23T12:16:16.203288Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.045676Z","iopub.execute_input":"2026-02-02T12:27:12.046486Z","iopub.status.idle":"2026-02-02T12:27:12.060549Z","shell.execute_reply.started":"2026-02-02T12:27:12.046453Z","shell.execute_reply":"2026-02-02T12:27:12.059689Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def build_initial_token_freqs(word_freqs, max_vocab=64000):\n    char_freqs = defaultdict(int)\n    subwords_freqs = defaultdict(int)\n    for word, freq in word_freqs.items():\n        for i in range(len(word)):\n            char_freqs[word[i]] += freq\n            # Loop through the subwords of length at least 2\n            for j in range(i + 2, len(word) + 1):\n                subwords_freqs[word[i:j]] += freq\n    \n    # Sort subwords by frequency\n    sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n\n    token_freqs = list(char_freqs.items()) + sorted_subwords[: max_vocab - len(char_freqs)]\n    token_freqs = {token: freq for token, freq in token_freqs}\n    return token_freqs","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:16:16.219348Z","start_time":"2026-01-23T12:16:16.215516Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.062032Z","iopub.execute_input":"2026-02-02T12:27:12.062301Z","iopub.status.idle":"2026-02-02T12:27:12.074151Z","shell.execute_reply.started":"2026-02-02T12:27:12.062276Z","shell.execute_reply":"2026-02-02T12:27:12.073192Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def encode_word(word, model):\n    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n        {\"start\": None, \"score\": None} for _ in range(len(word))\n    ]\n    for start_idx in range(len(word)):\n        # This should be properly filled by the previous steps of the loop\n        best_score_at_start = best_segmentations[start_idx][\"score\"]\n        for end_idx in range(start_idx + 1, len(word) + 1):\n            token = word[start_idx:end_idx]\n            if token in model and best_score_at_start is not None:\n                score = model[token] + best_score_at_start\n                # If we have found a better segmentation ending at end_idx, we update\n                if (\n                    best_segmentations[end_idx][\"score\"] is None\n                    or best_segmentations[end_idx][\"score\"] > score\n                ):\n                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n\n    segmentation = best_segmentations[-1]\n    if segmentation[\"score\"] is None:\n        # We did not find a tokenization of the word -> unknown\n        return [\"<unk>\"], 0\n\n    score = segmentation[\"score\"]\n    start = segmentation[\"start\"]\n    end = len(word)\n    tokens = []\n    while start != 0:\n        tokens.insert(0, word[start:end])\n        next_start = best_segmentations[start][\"start\"]\n        end = start\n        start = next_start\n    tokens.insert(0, word[start:end])\n    return tokens, score","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:18:25.364219Z","start_time":"2026-01-23T12:18:25.358715Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.075538Z","iopub.execute_input":"2026-02-02T12:27:12.075888Z","iopub.status.idle":"2026-02-02T12:27:12.095714Z","shell.execute_reply.started":"2026-02-02T12:27:12.075863Z","shell.execute_reply":"2026-02-02T12:27:12.094865Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def compute_loss(model, word_freqs):\n    loss = 0\n    for word, freq in word_freqs.items():\n        _, word_loss = encode_word(word, model)\n        loss += freq * word_loss\n    return loss","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:19:17.844831Z","start_time":"2026-01-23T12:19:17.841819Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.097068Z","iopub.execute_input":"2026-02-02T12:27:12.097518Z","iopub.status.idle":"2026-02-02T12:27:12.120435Z","shell.execute_reply.started":"2026-02-02T12:27:12.097463Z","shell.execute_reply":"2026-02-02T12:27:12.119339Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import copy\n\ndef compute_scores(model, word_freqs):\n    scores = {}\n    model_loss = compute_loss(model, word_freqs)\n\n    i = 0\n    for token in model.keys():\n\n        if i % 100 == 0:\n            print(i, \"/\", len(model.keys()))\n        i += 1\n        \n        # We always keep tokens of length 1\n        if len(token) == 1:\n            continue\n        model_without_token = dict(model)\n        model_without_token.pop(token, None)\n        scores[token] = compute_loss(model_without_token, word_freqs) - model_loss\n    return scores","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:21:52.337236Z","start_time":"2026-01-23T12:21:52.333734Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.121667Z","iopub.execute_input":"2026-02-02T12:27:12.122897Z","iopub.status.idle":"2026-02-02T12:27:12.137993Z","shell.execute_reply.started":"2026-02-02T12:27:12.122860Z","shell.execute_reply":"2026-02-02T12:27:12.137052Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"with open(\"/kaggle/input/court-full-final/tensorflow2/default/1/model_court.pkl\", \"rb\") as f:\n    model_court = pickle.load(f)\n\nwith open(\"/kaggle/input/social-full-final/tensorflow2/default/1/model_ social.pkl\", \"rb\") as f:\n    model_social = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.139173Z","iopub.execute_input":"2026-02-02T12:27:12.139517Z","iopub.status.idle":"2026-02-02T12:27:12.193431Z","shell.execute_reply.started":"2026-02-02T12:27:12.139484Z","shell.execute_reply":"2026-02-02T12:27:12.192468Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def tokenize(text, model):\n    pre_tokens = wpt.tokenize(str(text)) \n    encoded_words = [encode_word(tok, model)[0] for tok in pre_tokens]\n    return sum(encoded_words, [])","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:23:21.958751Z","start_time":"2026-01-23T12:23:21.955361Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:12.194476Z","iopub.execute_input":"2026-02-02T12:27:12.194815Z","iopub.status.idle":"2026-02-02T12:27:12.199826Z","shell.execute_reply.started":"2026-02-02T12:27:12.194778Z","shell.execute_reply":"2026-02-02T12:27:12.198963Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def eval_model(model, df):\n    n = 20000\n    total_words = 0\n    total_subwords = 0\n    total_unk = 0\n    \n    for i in range(n):\n        text = df[i]\n        words = wpt.tokenize(str(text))\n        subwords = tokenize(text, model)\n\n        total_words += max(1, len(words))\n        total_subwords += len(subwords)\n        total_unk += sum(1 for s in subwords if s == \"<unk>\")\n\n        unk = (total_unk / total_subwords) if total_subwords else 0.0\n        \n    return {\n        \"fertility_subwords_per_word\": f\"{total_subwords / total_words:.2f}\",\n        \"coverage\": f\"{(1 - unk)*100:.2f}\",\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:50.418319Z","iopub.execute_input":"2026-02-02T12:27:50.419174Z","iopub.status.idle":"2026-02-02T12:27:50.425621Z","shell.execute_reply.started":"2026-02-02T12:27:50.419139Z","shell.execute_reply":"2026-02-02T12:27:50.424767Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"court_val_texts = court_val[0].astype(str).tolist()\nsocial_val_texts = social_val[0].astype(str).tolist()\n\nprint(f\"model_court on court_val: {eval_model(model_court, court_val_texts)},\"\n      f\"\\n\\n\\nmodel_court on social_val: {eval_model(model_court, social_val_texts)},\"\n      f\"\\n\\n\\nmodel_social on court_val: {eval_model(model_social, court_val_texts)},\"\n      f\"\\n\\n\\nmodel_social on social_val: {eval_model(model_social, social_val_texts)}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:27:51.103777Z","iopub.execute_input":"2026-02-02T12:27:51.104103Z","iopub.status.idle":"2026-02-02T12:28:17.340362Z","shell.execute_reply.started":"2026-02-02T12:27:51.104077Z","shell.execute_reply":"2026-02-02T12:28:17.339623Z"}},"outputs":[{"name":"stdout","text":"model_court on court_val: {'fertility_subwords_per_word': '1.14', 'coverage': '100.00'},\n\n\nmodel_court on social_val: {'fertility_subwords_per_word': '2.03', 'coverage': '96.97'},\n\n\nmodel_social on court_val: {'fertility_subwords_per_word': '1.74', 'coverage': '100.00'},\n\n\nmodel_social on social_val: {'fertility_subwords_per_word': '1.64', 'coverage': '98.13'}.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(\"Origin:\", \"–¶–µ —Ç–µ—Å—Ç –¥–ª—è —Å—É–¥–æ–≤–æ–≥–æ —Ç–∞ —Å–æ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å—É—Ç—É, (Hello new model) üÜó\")\nprint()\nprint(\"model_court:\", tokenize(\"–¶–µ —Ç–µ—Å—Ç –¥–ª—è —Å—É–¥–æ–≤–æ–≥–æ —Ç–∞ —Å–æ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å—É—Ç—É, (Hello new model) üÜó\", model_court))\nprint()\nprint(\"model_social:\", tokenize(\"–¶–µ —Ç–µ—Å—Ç –¥–ª—è —Å—É–¥–æ–≤–æ–≥–æ —Ç–∞ —Å–æ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å—É—Ç—É, (Hello new model) üÜó\", model_social))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}}]}