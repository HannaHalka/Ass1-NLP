{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14592870,"sourceType":"datasetVersion","datasetId":9321449},{"sourceId":14642355,"sourceType":"datasetVersion","datasetId":9353664},{"sourceId":14643928,"sourceType":"datasetVersion","datasetId":9354574},{"sourceId":14644020,"sourceType":"datasetVersion","datasetId":9354616},{"sourceId":736781,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":561774,"modelId":574409}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T07:07:05.040749Z","iopub.execute_input":"2026-01-31T07:07:05.041082Z","iopub.status.idle":"2026-01-31T07:07:05.045227Z","shell.execute_reply.started":"2026-01-31T07:07:05.041058Z","shell.execute_reply":"2026-01-31T07:07:05.044622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"social = pd.read_csv(\"/kaggle/input/nlp-ass1-datasets/social.csv\", header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T07:11:55.532539Z","iopub.execute_input":"2026-01-31T07:11:55.532880Z","iopub.status.idle":"2026-01-31T07:11:55.854672Z","shell.execute_reply.started":"2026-01-31T07:11:55.532853Z","shell.execute_reply":"2026-01-31T07:11:55.854086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"social_train, social_val = social[:80000], social[80000:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T07:23:07.610682Z","iopub.execute_input":"2026-01-31T07:23:07.611431Z","iopub.status.idle":"2026-01-31T07:23:07.615218Z","shell.execute_reply.started":"2026-01-31T07:23:07.611400Z","shell.execute_reply":"2026-01-31T07:23:07.614450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\ndef deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS) \n    u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols\n    u\"\\U0001FA00-\\U0001FAFF\"  # symbols & pictographs ext\n    u\"\\u2600-\\u26FF\"          # misc symbols\n    u\"\\u2700-\\u27BF\"          # dingbats\n                       \"]+\", flags = re.UNICODE)\n\n    if not isinstance(text, str):\n        return text\n    text = regrex_pattern.sub(\"\", text)\n\n    text = text.replace(\"\\uFE0F\", \"\").replace(\"\\u200D\", \"\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T07:22:29.422029Z","iopub.execute_input":"2026-01-31T07:22:29.422352Z","iopub.status.idle":"2026-01-31T07:22:29.427916Z","shell.execute_reply.started":"2026-01-31T07:22:29.422328Z","shell.execute_reply":"2026-01-31T07:22:29.427304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_cols = social_train.select_dtypes(include=[\"object\"]).columns\nsocial_train[text_cols] = social_train[text_cols].applymap(deEmojify)\nsocial_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T07:22:43.372903Z","iopub.execute_input":"2026-01-31T07:22:43.373416Z","iopub.status.idle":"2026-01-31T07:22:43.697400Z","shell.execute_reply.started":"2026-01-31T07:22:43.373390Z","shell.execute_reply":"2026-01-31T07:22:43.696604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.tokenize import WordPunctTokenizer\n\nwpt = WordPunctTokenizer()","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:16:16.199112Z","start_time":"2026-01-23T12:16:12.047573Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.637636Z","iopub.execute_input":"2026-01-27T11:39:14.637902Z","iopub.status.idle":"2026-01-27T11:39:14.647993Z","shell.execute_reply.started":"2026-01-27T11:39:14.637871Z","shell.execute_reply":"2026-01-27T11:39:14.647362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\ndef build_word_freqs(corpus):\n    word_freqs = defaultdict(int)\n    for text in corpus:\n       for tok in wpt.tokenize(str(text)):\n           word_freqs[tok] += 1\n    return word_freqs","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:16:16.208951Z","start_time":"2026-01-23T12:16:16.203288Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.649730Z","iopub.execute_input":"2026-01-27T11:39:14.650301Z","iopub.status.idle":"2026-01-27T11:39:14.661311Z","shell.execute_reply.started":"2026-01-27T11:39:14.650280Z","shell.execute_reply":"2026-01-27T11:39:14.660606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_initial_token_freqs(word_freqs, max_vocab=64000):\n    char_freqs = defaultdict(int)\n    subwords_freqs = defaultdict(int)\n    for word, freq in word_freqs.items():\n        for i in range(len(word)):\n            char_freqs[word[i]] += freq\n            # Loop through the subwords of length at least 2\n            for j in range(i + 2, len(word) + 1):\n                subwords_freqs[word[i:j]] += freq\n    \n    # Sort subwords by frequency\n    sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n\n    token_freqs = list(char_freqs.items()) + sorted_subwords[: max_vocab - len(char_freqs)]\n    token_freqs = {token: freq for token, freq in token_freqs}\n    return token_freqs","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:16:16.219348Z","start_time":"2026-01-23T12:16:16.215516Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.662050Z","iopub.execute_input":"2026-01-27T11:39:14.662405Z","iopub.status.idle":"2026-01-27T11:39:14.673450Z","shell.execute_reply.started":"2026-01-27T11:39:14.662374Z","shell.execute_reply":"2026-01-27T11:39:14.672706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_word(word, model):\n    best_segmentations = [{\"start\": 0, \"score\": 0}] + [\n        {\"start\": None, \"score\": None} for _ in range(len(word))\n    ]\n    for start_idx in range(len(word)):\n        # This should be properly filled by the previous steps of the loop\n        best_score_at_start = best_segmentations[start_idx][\"score\"]\n        for end_idx in range(start_idx + 1, len(word) + 1):\n            token = word[start_idx:end_idx]\n            if token in model and best_score_at_start is not None:\n                score = model[token] + best_score_at_start\n                # If we have found a better segmentation ending at end_idx, we update\n                if (\n                    best_segmentations[end_idx][\"score\"] is None\n                    or best_segmentations[end_idx][\"score\"] > score\n                ):\n                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n\n    segmentation = best_segmentations[-1]\n    if segmentation[\"score\"] is None:\n        # We did not find a tokenization of the word -> unknown\n        return [\"<unk>\"], 0\n\n    score = segmentation[\"score\"]\n    start = segmentation[\"start\"]\n    end = len(word)\n    tokens = []\n    while start != 0:\n        tokens.insert(0, word[start:end])\n        next_start = best_segmentations[start][\"start\"]\n        end = start\n        start = next_start\n    tokens.insert(0, word[start:end])\n    return tokens, score","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:18:25.364219Z","start_time":"2026-01-23T12:18:25.358715Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.674273Z","iopub.execute_input":"2026-01-27T11:39:14.674512Z","iopub.status.idle":"2026-01-27T11:39:14.681891Z","shell.execute_reply.started":"2026-01-27T11:39:14.674485Z","shell.execute_reply":"2026-01-27T11:39:14.681202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_loss(model, word_freqs):\n    loss = 0\n    for word, freq in word_freqs.items():\n        _, word_loss = encode_word(word, model)\n        loss += freq * word_loss\n    return loss","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:19:17.844831Z","start_time":"2026-01-23T12:19:17.841819Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.682775Z","iopub.execute_input":"2026-01-27T11:39:14.682991Z","iopub.status.idle":"2026-01-27T11:39:14.696983Z","shell.execute_reply.started":"2026-01-27T11:39:14.682972Z","shell.execute_reply":"2026-01-27T11:39:14.696385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\n\ndef compute_scores(model, word_freqs):\n    scores = {}\n    model_loss = compute_loss(model, word_freqs)\n\n    i = 0\n    for token in model.keys():\n\n        if i % 100 == 0:\n            print(i, \"/\", len(model.keys()))\n        i += 1\n        \n        # We always keep tokens of length 1\n        if len(token) == 1:\n            continue\n        model_without_token = dict(model)\n        model_without_token.pop(token, None)\n        scores[token] = compute_loss(model_without_token, word_freqs) - model_loss\n    return scores","metadata":{"ExecuteTime":{"end_time":"2026-01-23T12:21:52.337236Z","start_time":"2026-01-23T12:21:52.333734Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.697937Z","iopub.execute_input":"2026-01-27T11:39:14.698271Z","iopub.status.idle":"2026-01-27T11:39:14.709221Z","shell.execute_reply.started":"2026-01-27T11:39:14.698250Z","shell.execute_reply":"2026-01-27T11:39:14.708457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from math import log\n\ndef unigram_model(df, init_vocab=40000, target_vocab=30000):\n    word_freqs = build_word_freqs(df)\n    token_freqs = build_initial_token_freqs(word_freqs, max_vocab=init_vocab)\n\n    total_sum = sum(token_freqs.values())\n    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n\n    print(\"Start Unigram model\")\n    percent_to_remove = 0.2\n    while len(model) > target_vocab:\n        print(len(model))\n        scores = compute_scores(model, word_freqs) ### <- somthing wrong\n        sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n        # Remove percent_to_remove tokens with the lowest scores.\n        for i in range(int(len(model) * percent_to_remove)):\n            _ = token_freqs.pop(sorted_scores[i][0])\n\n        total_sum = sum([freq for token, freq in token_freqs.items()])\n        model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n\n        with open(\"model_social.pkl\", \"wb\") as f:\n            pickle.dump(model, f)\n            print(\"model saved vocab size: \", len(model))\n        \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.710280Z","iopub.execute_input":"2026-01-27T11:39:14.710944Z","iopub.status.idle":"2026-01-27T11:39:14.724516Z","shell.execute_reply.started":"2026-01-27T11:39:14.710910Z","shell.execute_reply":"2026-01-27T11:39:14.723886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"social_corpus = social_train[0].astype(str).tolist()\nmodel_social = unigram_model(social_corpus)\nprint(\"social model size:\", len(model_social))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T11:39:14.725374Z","iopub.execute_input":"2026-01-27T11:39:14.725669Z","iopub.status.idle":"2026-01-27T11:40:57.018475Z","shell.execute_reply.started":"2026-01-27T11:39:14.725637Z","shell.execute_reply":"2026-01-27T11:40:57.017250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"model_social.pkl\", \"wb\") as f:\n    pickle.dump(model_social, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"model model_social.pkl saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}